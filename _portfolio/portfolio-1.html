---
title: "Enhancing Authorship Attribution through LLM-Based Textual Mimicry"
excerpt: "Exploring the interplay between language models and authorship detection using reinforcement learning.<br/><img src='/images/authorship_attribution_image.png' width="500" height="500">"
collection: portfolio
---

<p>
This project investigates the ability of large language models (LLMs) to generate textual forgeries capable of deceiving a robust authorship classifier. By employing reinforcement learning techniques such as Proximal Policy Optimization (PPO), we iteratively improve the LLM’s capability to mimic an author's writing style while simultaneously analyzing the classifier’s resilience. The experiment utilizes texts from Jane Austen's novels and synthetic samples generated by Llama3-8b.
</p>

<h2>Code</h2>

<p><a href="https://github.com/clu-ling/dugmore-detector/tree/julian-rambob/src/clu/authorship">Link to GitHub Repository</a></p>

<h2>Goals</h2>

<p>
The primary objective of this experiment is to enhance an LLM’s ability to generate deceptive textual forgeries. As the model improves, we aim to assess the classifier’s robustness over multiple fine-tuning iterations. The ultimate goal is to determine whether classifier accuracy declines as the LLM becomes a more effective mimic. This research provides insights into both the vulnerabilities of authorship attribution models and the capacity of LLMs to refine their stylistic imitation through reinforcement learning.
</p>

<h2>Phase 1: Training and Evaluation</h2>

<h3>Corpus Preparation</h3>

<p>
To build the dataset, we collect the full texts of six Jane Austen novels: <em>Emma</em>, <em>Mansfield Park</em>, <em>Northanger Abbey</em>, <em>Persuasion</em>, <em>Sense and Sensibility</em>, and <em>Pride and Prejudice</em>. Each text is segmented into two-sentence passages, which serve as prompts for the LLM. Alongside the text, we preserve metadata such as the author, title, and passage location to track generated outputs effectively.
</p>

<h3>Data Splitting</h3>

<p>
The corpus is split into training and evaluation sets. Four novels are used for training, while the remaining two are reserved for evaluation. Using these passages as input, we prompt Llama3-8b to generate stylistic continuations that attempt to mimic Austen’s writing. These generated texts are labeled as "mimic," while the original Austen passages are labeled as "author."
</p>

<h3>Classifier Training</h3>

<p>
A logistic regression classifier is trained to distinguish between authentic Austen texts and LLM-generated forgeries. The classifier’s performance is evaluated using a held-out portion of the training data, where we measure its prediction probabilities on both real and mimic texts. This baseline performance serves as a reference for subsequent iterations of fine-tuning.
</p>

<h2>Phase 2: PPO Reinforcement Learning (Repeatable)</h2>

<h3>Classifier-Guided Fine-Tuning</h3>

<p>
To improve mimicry, we leverage the classifier’s output probabilities to rank generated texts. Mimic texts that receive high probabilities of being real are treated as preferred responses, while those with lower probabilities are treated as non-preferred. Using this feedback, we fine-tune Llama3-8b through Direct Preference Optimization (DPO) via Hugging Face, enabling the model to generate increasingly deceptive text.
</p>

<h3>Iterative Experimentation</h3>

<p>
After fine-tuning, the newly optimized model is used to regenerate mimic texts for both the training and evaluation sets. The classifier is then retrained on this updated data, and its performance is compared against previous iterations. As this process repeats, we monitor whether the classifier degrades in accuracy as the LLM continues to refine its imitation abilities.
</p>

<h2>Phase 3: Iterative Classifier Improvement</h2>

<p>
Beyond degrading classifier performance, another key goal is to explore strategies for improving classifier robustness. We iteratively retrain the classifier using the best deceptive samples from prior iterations, gradually increasing its ability to differentiate between real and generated texts. This process creates an adversarial loop, where the LLM and classifier co-evolve in a manner similar to a Generative Adversarial Network (GAN), with the LLM acting as the generator and the classifier as the discriminator.
</p>

<h2>Results</h2>

<p>
  Pending
</p>

<h2>Publications</h2>

<p>Pending</p>

<h2>Future Directions</h2>

<p>
This experiment demonstrates the potential of reinforcement learning to enhance textual mimicry while revealing vulnerabilities in existing authorship attribution models. Future research directions include exploring transformer-based classifiers, refining feature selection, and extending the methodology to other literary styles, such as academic writing. The insights gained from this workflow could contribute to advancements in authorship detection, LLM customization, and broader machine-learning applications in text generation and analysis.
</p>
